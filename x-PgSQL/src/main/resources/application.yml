server:
  port: 8084
spring:
  kafka:
    listener:
      ack-mode: manual_immediate # 手动确认模式
    bootstrap-servers: localhost:9092
    producer:
      transaction-id-prefix: tx-order- # 添加事务 ID 前缀
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
    consumer:
      group-id: fund-group
      auto-offset-reset: earliest  # 从最早消费，测试时用；生产用 latest
      max-poll-records: 1  # 避免批量失败
#    session-timeout-ms: 30000  # 延长心跳
      enable-auto-commit: false # 关闭自动提交,需手动提交事务,确保exactly-once,如果你想手动调用 Acknowledgment.acknowledge() 提交偏移量，确保消息消费一次且仅一次。
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      properties:
        retries: 3  # 全局重试次数
        retry-backoff-ms: 1000
#        spring.json.value.default.type: com.example.dto.OrderPaidEvent
#        spring.json.trusted.packages: xCloud.entity.PaymentSuccessEvent
        spring.json.trusted.packages: "*"
#  sql:
#    init:
#      data-locations: classpath:data.sql
#      mode: ALWAYS
#      schema-locations: classpath:schema.sql
  #  数据库配置
  datasource:
    type: com.alibaba.druid.pool.DruidDataSource
    driver-class-name: org.postgresql.Driver  # PostgreSQL 驱动
    url: jdbc:postgresql://db.mlbypadufgwqslopkqwy.supabase.co:5432/postgres?sslmode=require
    username: postgres
    password: ANDY@345&kl;'
    #    url: jdbc:postgresql://localhost:5432/postgres?useUnicode=true&characterEncoding=utf8&useSSL=false&serverTimezone=UTC
    #    username: andy_mac
#    password: hy123456

    druid:
      initial-size: 5
      min-idle: 5
      max-active: 20
      max-wait: 60000
      test-on-borrow: true
      test-on-return: false
      test-while-idle: true
      #是否启用 Druid 的监控统计页面（StatViewServlet）。【访问 http://localhost:8080/druid】Druid 监控：启动后访问 http://localhost:8080/druid/（登录 admin/admin），查看连接池状态
#      如 Spring Boot 3.x 需要额外配置「启动类加：@ServletComponentScan // 启用 Servlet 组件扫描（Druid 的 StatViewServlet 需要）」
      filters: stat,wall,slf4j # 启用 SQL 统计功能、防火墙、日志
      filter:
        stat:
          enabled: true
          slow-sql-millis: 1000 # 慢 SQL 阈值（毫秒）
        wall:
          enabled: true
      stat-view-servlet:
        enabled: true
        url-pattern: /druid/*
        login-username: andy
        login-password: andy
        # 关闭登录验证（不推荐生产环境使用）
        allow: 127.0.0.1  # 允许的 IP（空表示允许所有）
#        deny: 192.168.1.100 # 拒绝的 IP
#    sql:
#      init:
#        mode: always # always: 每次应用启动时都执行指定的 SQL 脚本（schema.sql 和 data.sql），即使数据库已经存在。  never: 不执行任何初始化脚本/embedded: 仅对嵌入式数据库（如 H2）执行脚本。
#        schema-locations: classpath:schema.sql
#        data-locations: classpath:data.sql
#SQL 初始化功能，用于在应用启动时自动执行 SQL 脚本以初始化数据库结构和数据

# mybatis-plus 配置
mybatis-plus:
  mapper-locations: classpath*:/mapper/**/*.xml
  type-aliases-package: xCloud.entity
  #  配置mybatis-plus日志必须1
  configuration:
#    打印 SQL 日志
    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl
#    MyBatis-Plus 默认启用下划线转驼峰命名
    map-underscore-to-camel-case: true

# 日志配置log---
#logging.level.xCloud.openAiChatModel.orchestrator.RagClient=DEBUG
logging:
  level:
    # 开启kafka调试日志，观察 poll/assign,重启应用，查看日志中 "Partitions assigned" 和 "Consumed message"。
    org.springframework.kafka: DEBUG

    # nacos日志，查找Nacos连接或配置加载的错误信息
    com.alibaba.nacos: DEBUG
    #    org.springframework.cloud.alibaba.nacos: DEBUG
    org.springframework.cloud: DEBUG
    io.grpc: DEBUG
    org.springframework.web: DEBUG
    #  配置mybatis-plus日志必须
    com.baomidou.mybatisplus: DEBUG

# Activiti / Flowable / Activiti7 可能有不同配置项。下面是 Activiti 7 的示例属性，可根据 starter 变更。
activiti:
  database-schema-update: true
  history-level: full
